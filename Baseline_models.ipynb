{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27eec699",
   "metadata": {},
   "source": [
    "## Load and clean every important files\n",
    "\n",
    "*Applying `eval` to each entry in the corresponding column, converting string representations of Python literals into their actual Python objects.*\n",
    "\n",
    "Here, cleaning is quite straightforward: it is just about removing null values and duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022d0698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading big and small matrices...\n",
      "Loading social network...\n",
      "Loading item features...\n",
      "Loading user features...\n",
      "Loading items' daily features...\n",
      "All data loaded.\n",
      "Cleaning data...\n",
      "Data cleaned.\n",
      "Big matrix: 7.71% cleaned\n",
      "Small matrix: 3.89% cleaned\n",
      "Social network: 0.00% cleaned\n",
      "Item categories: 0.00% cleaned\n",
      "User features: 3.86% cleaned\n",
      "Item daily features: 30.11% cleaned\n"
     ]
    }
   ],
   "source": [
    "from load_clean import load_and_clean_data\n",
    "\n",
    "big_matrix, small_matrix, social_network, item_categories, user_features, item_daily_features = load_and_clean_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ed02d",
   "metadata": {},
   "source": [
    "## Binarize data\n",
    "\n",
    "We consider an interaction is positive when `watch_ratio >= 2`.\n",
    "\n",
    "This comes from the KuaiRec paper, \"A user–video pair is considered positive if the user’s cumulative watch time is at least twice the video’s duration (i.e. watch_ratio >= 2).\n",
    "This threshold ensures we capture strong signals (rewatches or full watches), and ignore casual or accidental views.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4691c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big  : (11564987, 9)   positives: 3925531\n",
      "Small: (4494578, 9)   positives: 1471862\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>play_duration</th>\n",
       "      <th>video_duration</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>watch_ratio</th>\n",
       "      <th>interaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3649</td>\n",
       "      <td>13838</td>\n",
       "      <td>10867</td>\n",
       "      <td>2020-07-05 00:08:23.438</td>\n",
       "      <td>20200705</td>\n",
       "      <td>1.593879e+09</td>\n",
       "      <td>1.273397</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>9598</td>\n",
       "      <td>13665</td>\n",
       "      <td>10984</td>\n",
       "      <td>2020-07-05 00:13:41.297</td>\n",
       "      <td>20200705</td>\n",
       "      <td>1.593879e+09</td>\n",
       "      <td>1.244082</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5262</td>\n",
       "      <td>851</td>\n",
       "      <td>7908</td>\n",
       "      <td>2020-07-05 00:16:06.687</td>\n",
       "      <td>20200705</td>\n",
       "      <td>1.593879e+09</td>\n",
       "      <td>0.107613</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1963</td>\n",
       "      <td>862</td>\n",
       "      <td>9590</td>\n",
       "      <td>2020-07-05 00:20:26.792</td>\n",
       "      <td>20200705</td>\n",
       "      <td>1.593880e+09</td>\n",
       "      <td>0.089885</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8234</td>\n",
       "      <td>858</td>\n",
       "      <td>11000</td>\n",
       "      <td>2020-07-05 00:43:05.128</td>\n",
       "      <td>20200705</td>\n",
       "      <td>1.593881e+09</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  video_id  play_duration  video_duration                     time  \\\n",
       "0        0      3649          13838           10867  2020-07-05 00:08:23.438   \n",
       "1        0      9598          13665           10984  2020-07-05 00:13:41.297   \n",
       "2        0      5262            851            7908  2020-07-05 00:16:06.687   \n",
       "3        0      1963            862            9590  2020-07-05 00:20:26.792   \n",
       "4        0      8234            858           11000  2020-07-05 00:43:05.128   \n",
       "\n",
       "       date     timestamp  watch_ratio  interaction  \n",
       "0  20200705  1.593879e+09     1.273397            1  \n",
       "1  20200705  1.593879e+09     1.244082            1  \n",
       "2  20200705  1.593879e+09     0.107613            0  \n",
       "3  20200705  1.593880e+09     0.089885            0  \n",
       "4  20200705  1.593881e+09     0.078000            0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for df in (big_matrix, small_matrix):\n",
    "    df['interaction'] = (df['watch_ratio'] >= 2).astype(int)\n",
    "\n",
    "print(\"Big  :\", big_matrix.shape, \"  positives:\", big_matrix['interaction'].sum())\n",
    "print(\"Small:\", small_matrix.shape, \"  positives:\", small_matrix['interaction'].sum())\n",
    "big_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed174a37",
   "metadata": {},
   "source": [
    "## 2. Popularity Recommender\n",
    "\n",
    "A super-simple baseline:  \n",
    "1.  Count how many times each `video_id` was **positively** watched in the **train** set.  \n",
    "2.  For each user, recommend the top-K most popular videos they **haven’t** already seen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e2bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "popularity = (\n",
    "    big_matrix[big_matrix['interaction']==1]\n",
    "      .groupby('video_id')['interaction']\n",
    "      .sum()\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "pop_list = popularity.index.tolist()\n",
    "\n",
    "seen = big_matrix[big_matrix['interaction']==1].groupby('user_id')['video_id'].apply(set).to_dict()\n",
    "\n",
    "def recommend_pop(user_id, K=10):\n",
    "    recs = []\n",
    "    seen_u = seen.get(user_id, set())\n",
    "    for vid in pop_list:\n",
    "        if vid not in seen_u:\n",
    "            recs.append(vid)\n",
    "        if len(recs)==K:\n",
    "            break\n",
    "    return recs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e475c586",
   "metadata": {},
   "source": [
    "## 3. Matrix Factorization with SVD\n",
    "\n",
    "We use the [`Surprise`](https://surprise.readthedocs.io/) library’s SVD on our **implicit** binary data:\n",
    "- Treat `interaction` (0/1) as the “rating.”  \n",
    "- 5-fold CV for RMSE/MAE.  \n",
    "- Then fit on the full train set and produce top-K scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b899f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.3999  0.4000  0.4001  0.3996  0.3995  0.3998  0.0002  \n",
      "MAE (testset)     0.3293  0.3297  0.3299  0.3292  0.3293  0.3295  0.0003  \n",
      "Fit time          94.88   136.84  121.53  150.66  140.33  128.85  19.38   \n",
      "Test time         56.89   78.09   42.73   56.46   57.80   58.40   11.31   \n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x1037af820>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install surprise if needed\n",
    "# !pip install scikit-surprise\n",
    "\n",
    "from surprise import Dataset, Reader, SVD\n",
    "#IMPORTANT: if you get an error about the version of numpy, try to downgrade it with 'pip uninstall numpy && pip install \"numpy<2.0\"'\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# 3.1 load into Surprise\n",
    "reader = Reader(rating_scale=(0,1))\n",
    "train_data = Dataset.load_from_df(big_matrix[['user_id','video_id','interaction']], reader)\n",
    "\n",
    "# 3.2 5-fold CV\n",
    "algo = SVD(n_factors=50, n_epochs=10, verbose=True)\n",
    "cv_results = cross_validate(algo, train_data, measures=['RMSE','MAE'], cv=5, verbose=True)\n",
    "\n",
    "# 3.3 fit on all train data\n",
    "trainset = train_data.build_full_trainset()\n",
    "algo.fit(trainset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906b31e",
   "metadata": {},
   "source": [
    "## 4. Evaluation: Ranking Metrics\n",
    "\n",
    "We now define:\n",
    "- **Precision@K**  \n",
    "- **Recall@K**  \n",
    "- **NDCG@K**  \n",
    "- **MAP@K**  \n",
    "- **Accuracy** (fraction of correctly predicted interactions over all test pairs, thresholding the SVD score at 0.5)\n",
    "\n",
    "and a helper to get **top-K** recommendations from our SVD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51da2c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 4.1 helper: top-K from SVD\n",
    "all_items = big_matrix['video_id'].unique().tolist()\n",
    "def recommend_svd(user_id, K=10):\n",
    "    scores = [ (iid, algo.predict(user_id, iid).est) for iid in all_items ]\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [iid for iid,_ in scores[:K]]\n",
    "\n",
    "# 4.2 metric functions\n",
    "def precision_at_k(recs, actual, K):\n",
    "    return len(set(recs[:K]) & set(actual)) / K\n",
    "\n",
    "def recall_at_k(recs, actual, K):\n",
    "    if not actual: return 0.0\n",
    "    return len(set(recs[:K]) & set(actual)) / len(actual)\n",
    "\n",
    "def dcg_at_k(recs, actual, K):\n",
    "    return sum((1 if rec in actual else 0)/np.log2(idx+2)\n",
    "               for idx, rec in enumerate(recs[:K]))\n",
    "\n",
    "def ndcg_at_k(recs, actual, K):\n",
    "    ideal = dcg_at_k(actual, actual, min(K,len(actual)))\n",
    "    return dcg_at_k(recs, actual, K) / ideal if ideal>0 else 0.0\n",
    "\n",
    "def average_precision(recs, actual, K):\n",
    "    hits, sum_prec = 0, 0.0\n",
    "    for i, r in enumerate(recs[:K]):\n",
    "        if r in actual:\n",
    "            hits += 1\n",
    "            sum_prec += hits/(i+1)\n",
    "    return sum_prec / min(len(actual), K) if actual else 0.0\n",
    "\n",
    "# 4.3 accuracy: predict interaction if score >= 0.5\n",
    "def accuracy_svd(test_df):\n",
    "    preds = [ algo.predict(u, v).est>=0.5\n",
    "              for u,v in zip(test_df.user_id, test_df.video_id) ]\n",
    "    return np.mean((test_df.interaction==1) == preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6285800",
   "metadata": {},
   "source": [
    "## 5. Evaluate on the **small_matrix** (test)\n",
    "\n",
    "For **each user** in `small`:\n",
    "1.  Collect the list of **positives** in the test set.  \n",
    "2.  Generate top-K recs from **Popularity** and **SVD**.  \n",
    "3.  Compute all metrics.  \n",
    "4.  Average across users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fec95e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating @K=1: 100%|██████████| 1411/1411 [00:33<00:00, 42.29it/s]\n",
      "Evaluating @K=10: 100%|██████████| 1411/1411 [00:32<00:00, 42.93it/s]\n",
      "Evaluating @K=100: 100%|██████████| 1411/1411 [00:36<00:00, 39.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results @K=1 ===\n",
      "-- POP  --\n",
      "Precision@1: 0.7052\n",
      "Recall   @1: 0.0007\n",
      "NDCG     @1: 0.7052\n",
      "MAP      @1: 0.7052\n",
      "-- SVD  --\n",
      "Precision@1: 0.0191\n",
      "Recall   @1: 0.0000\n",
      "NDCG     @1: 0.0191\n",
      "MAP      @1: 0.0191\n",
      "\n",
      "=== Results @K=10 ===\n",
      "-- POP  --\n",
      "Precision@10: 0.7587\n",
      "Recall   @10: 0.0080\n",
      "NDCG     @10: 0.7536\n",
      "MAP      @10: 0.6419\n",
      "-- SVD  --\n",
      "Precision@10: 0.0189\n",
      "Recall   @10: 0.0001\n",
      "NDCG     @10: 0.0187\n",
      "MAP      @10: 0.0091\n",
      "\n",
      "=== Results @K=100 ===\n",
      "-- POP  --\n",
      "Precision@100: 0.6947\n",
      "Recall   @100: 0.0723\n",
      "NDCG     @100: 0.7040\n",
      "MAP      @100: 0.5280\n",
      "-- SVD  --\n",
      "Precision@100: 0.0121\n",
      "Recall   @100: 0.0013\n",
      "NDCG     @100: 0.0129\n",
      "MAP      @100: 0.0024\n",
      "\n",
      "Overall SVD Accuracy on test set: 0.7688445945314555\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "Ks = [1, 10, 100]\n",
    "\n",
    "users   = small_matrix['user_id'].unique()\n",
    "actuals = (small_matrix[small_matrix['interaction']==1]\n",
    "           .groupby('user_id')['video_id']\n",
    "           .apply(list)\n",
    "           .to_dict())\n",
    "\n",
    "# prepare a metrics dict: metrics[K][model][metric_name] = list of scores\n",
    "metrics = {\n",
    "    K: {\n",
    "        'pop': {'prec': [], 'rec': [], 'ndcg': [], 'map': []},\n",
    "        'svd': {'prec': [], 'rec': [], 'ndcg': [], 'map': []}\n",
    "    } for K in Ks\n",
    "}\n",
    "\n",
    "for K in Ks:\n",
    "    for u in tqdm(users, desc=f\"Evaluating @K={K}\"):\n",
    "        act      = actuals.get(u, [])\n",
    "        pop_recs = recommend_pop(u, K)\n",
    "        svd_recs = recommend_svd(u, K)\n",
    "\n",
    "        # populate pop metrics\n",
    "        metrics[K]['pop']['prec'].append( precision_at_k(pop_recs,    act, K) )\n",
    "        metrics[K]['pop']['rec'].append(  recall_at_k(pop_recs,       act, K) )\n",
    "        metrics[K]['pop']['ndcg'].append(ndcg_at_k(pop_recs,          act, K) )\n",
    "        metrics[K]['pop']['map'].append(average_precision(pop_recs,   act, K) )\n",
    "\n",
    "        # populate svd metrics\n",
    "        metrics[K]['svd']['prec'].append( precision_at_k(svd_recs,    act, K) )\n",
    "        metrics[K]['svd']['rec'].append(  recall_at_k(svd_recs,       act, K) )\n",
    "        metrics[K]['svd']['ndcg'].append(ndcg_at_k(svd_recs,          act, K) )\n",
    "        metrics[K]['svd']['map'].append(average_precision(svd_recs,   act, K) )\n",
    "\n",
    "for K in Ks:\n",
    "    print(f\"\\n=== Results @K={K} ===\")\n",
    "    for model in ('pop', 'svd'):\n",
    "        print(f\"-- {model.upper():4s} --\")\n",
    "        print(f\"Precision@{K}: {np.mean(metrics[K][model]['prec']):.4f}\")\n",
    "        print(f\"Recall   @{K}: {np.mean(metrics[K][model]['rec']):.4f}\")\n",
    "        print(f\"NDCG     @{K}: {np.mean(metrics[K][model]['ndcg']):.4f}\")\n",
    "        print(f\"MAP      @{K}: {np.mean(metrics[K][model]['map']):.4f}\")\n",
    "\n",
    "print(\"\\nOverall SVD Accuracy on test set:\", accuracy_svd(small_matrix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d7f97",
   "metadata": {},
   "source": [
    "The baseline results reveal a stark contrast between a trivial popularity heuristic and a standard SVD model:  \n",
    "- The **Popularity** recommender achieves deceptively high **Precision@K** and **NDCG@K** by repeatedly surfacing the same handful of hits, yet its **Recall** is essentially zero in the face of dozens—or hundreds—of true positives per user.  \n",
    "- Conversely, vanilla **SVD** barely recovers any relevant items in the top-K, yielding both low precision and near-zero recall despite an overall accuracy of ~77% (a meaningless number given the imbalance of negatives vs. positives).  \n",
    "\n",
    "In short, **high precision** here is driven by a tiny set of “easy” hits, while **low recall** exposes that nearly all genuine preferences lie outside the narrow top-K window. These findings underscore two fundamental limitations: (1) evaluating at small K hides the bulk of user interests, and (2) classical pointwise factorization is ill-suited to capture the long tail of positive signals.  \n",
    "\n",
    "**Key takeaways & next steps**  \n",
    "- Adopt ranking-centric or implicit-feedback methods (e.g. BPR, ALS) that directly optimize top-N retrieval.  \n",
    "- Incorporate side information (video tags, user features, social graph) or hard-negative sampling to diversify recommendations and boost recall without sacrificing precision.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
